{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word-Embedding-Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euler16/Exploration/blob/master/Word_Embedding_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFqAt9VJdW2w",
        "colab_type": "code",
        "outputId": "6492b081-2dd3-447d-c57b-a9b52518db1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# Downloading fasttext word vectors\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-18 22:03:00--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  5.55MB/s    in 97s     \n",
            "\n",
            "2019-11-18 22:04:38 (6.73 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n",
            "sample_data  wiki-news-300d-1M.vec  wiki-news-300d-1M.vec.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiCCkctiCdAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import gzip\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import gensim\n",
        "from copy import deepcopy\n",
        "\n",
        "#for tables in Jupyter\n",
        "from IPython.display import HTML, display\n",
        "import tabulate\n",
        "\n",
        "\n",
        "# forgetting about visualization at the moment\n",
        "from gensim import models\n",
        "from sklearn.decomposition import PCA as sPCA\n",
        "from sklearn import manifold #MSD, t-SNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVrwIAcZ5ng8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "isNumber = re.compile(r'\\d+.*')\n",
        "def norm_word(word):\n",
        "  if isNumber.search(word.lower()):\n",
        "    return '---num---'\n",
        "  elif re.sub(r'\\W+', '', word) == '':\n",
        "    return '---punc---'\n",
        "  else:\n",
        "    return word.lower()\n",
        "\n",
        "def read_lexicon(filename):\n",
        "  lexicon = {}\n",
        "  for line in open(filename, 'r'):\n",
        "    words = line.lower().strip().split()\n",
        "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
        "  return lexicon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc-S-FgSDDVD",
        "colab_type": "code",
        "outputId": "4bf1aa9f-1184-4e2d-ae0b-c4d94df89634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load word vectors\n",
        "\n",
        "def read_word_vecs(filename='wiki-news-300d-1M.vec'):\n",
        "  wordVectors = {}\n",
        "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
        "  else: fileObject = open(filename, 'r')\n",
        "  fileObject.readline()\n",
        "  for line in fileObject:\n",
        "    line = line.strip().lower()\n",
        "    word = line.split()[0]\n",
        "    wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
        "    for index, vecVal in enumerate(line.split()[1:]):\n",
        "      wordVectors[word][index] = float(vecVal)\n",
        "    ''' normalize weight vector '''\n",
        "    wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
        "    \n",
        "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
        "  return wordVectors\n",
        "\n",
        "wv = read_word_vecs()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors read from: wiki-news-300d-1M.vec \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNe2OSQN73MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrofitting code\n",
        "def retrofit(wordVecs, lexicon, numIters):\n",
        "  newWordVecs = deepcopy(wordVecs)\n",
        "  wvVocab = set(newWordVecs.keys())\n",
        "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
        "  for it in range(numIters):\n",
        "    # loop through every node also in ontology (else just use data estimate)\n",
        "    for word in loopVocab:\n",
        "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
        "      numNeighbours = len(wordNeighbours)\n",
        "      #no neighbours, pass - use data estimate\n",
        "      if numNeighbours == 0:\n",
        "        continue\n",
        "      # the weight of the data estimate if the number of neighbours\n",
        "      newVec = numNeighbours * wordVecs[word]\n",
        "      # loop over neighbours and add to new vector (currently with weight 1)\n",
        "      for ppWord in wordNeighbours:\n",
        "        newVec += newWordVecs[ppWord]\n",
        "      newWordVecs[word] = newVec/(2*numNeighbours)\n",
        "  return newWordVecs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXlx6I144BUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "db834558-4c2d-4854-b2ef-319870f2601b"
      },
      "source": [
        "# getting the lexicon\n",
        "!wget https://github.com/mfaruqui/retrofitting/raw/master/lexicons/ppdb-xl.txt\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-18 22:18:36--  https://github.com/mfaruqui/retrofitting/raw/master/lexicons/ppdb-xl.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mfaruqui/retrofitting/master/lexicons/ppdb-xl.txt [following]\n",
            "--2019-11-18 22:18:37--  https://raw.githubusercontent.com/mfaruqui/retrofitting/master/lexicons/ppdb-xl.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3948382 (3.8M) [text/plain]\n",
            "Saving to: ‘ppdb-xl.txt.1’\n",
            "\n",
            "ppdb-xl.txt.1       100%[===================>]   3.76M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-11-18 22:18:38 (45.1 MB/s) - ‘ppdb-xl.txt.1’ saved [3948382/3948382]\n",
            "\n",
            "ppdb-xl.txt    sample_data\t      wiki-news-300d-1M.vec.zip\n",
            "ppdb-xl.txt.1  wiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZhTxJEd4yXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lexicon = read_lexicon('ppdb-xl.txt')\n",
        "wv_retro = retrofit(wv, lexicon, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58uQnrsd5U5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7b6a4d0-bc1a-4d4a-cdaf-aed987c76d18"
      },
      "source": [
        "sum([np.sum(wv[x]-wv_retro[x]) for x in wv.keys()])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5913.370200771669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RIrDpEM6qKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating distortion\n",
        "import itertools\n",
        "from random import random\n",
        "def distortion(wv, wv_retro):\n",
        "  '''\n",
        "    expansion = max(d(f(x),f(y))/d(x,y))\n",
        "  '''\n",
        "  delta = 0.00000001\n",
        "  cp = list(itertools.product(list(wv.keys()), list(wv_retro.keys())))\n",
        "  cp = ((x,y) for x,y in cp if x != y)\n",
        "  cp = random.shuffle(cp)[:1000]\n",
        "  e =  0.0\n",
        "  c = 0.0\n",
        "  tmp = np.empty_like(wv[list(wv.keys())[0]])\n",
        "  tmp2 = np.empty_like(wv[list(wv.keys())[0]])\n",
        "  for x,y in cp:\n",
        "    np.square(wv_retro[x]-wv_retro[y], out=tmp)\n",
        "    np.sum(tmp, out=tmp)\n",
        "    np.sqrt(tmp, out=tmp)\n",
        "\n",
        "    np.square(wv[x]-wv[y], out=tmp2)\n",
        "    np.sum(tmp2, out=tmp2)\n",
        "    np.sqrt(tmp2, out=tmp2)\n",
        "\n",
        "    efrac = tmp /(tmp2 + delta)\n",
        "    cfrac = tmp2 /(tmp + delta)\n",
        "\n",
        "    e = max(e, efrac)\n",
        "    c = max(c, cfrac)\n",
        "\n",
        "  return e*c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtNsXIdASfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distortion(wv, wv_retro)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2zDGcZJAXTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}